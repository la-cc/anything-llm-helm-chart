###### Anything-LLM ######
# -- Override the full name of the chart.
fullnameOverride: "anything-llm"

# -- Number of pod replicas to deploy.
replicaCount: 1

# -- Configuration for the Docker image used by the pod.
image:
  # -- The Docker repository to pull the image from.
  repository: ghcr.io/mintplex-labs/anything-llm
  # -- The specific tag of the image to use.
  tag: 1.2.2
  # -- The pull policy for the image. IfNotPresent means the image will only be pulled if it is not already present locally.
  pullPolicy: IfNotPresent

# -- Service configuration.
service:
  # -- Service type.
  type: ClusterIP
  # -- Service port.
  port: 3001

# -- Ingress configuration.
ingress:
  # -- Enable ingress.
  enabled: true
  # -- Ingress annotations.
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    cert-manager.io/cluster-issuer: letsencrypt-dns
    cert-manager.io/renew-before: 360h
  # -- Ingress class name.
  ingressClassName: nginx
  # -- Ingress hosts.
  hosts:
    - host: llm.example.com
      paths:
        - path: /
          pathType: Prefix
  # -- TLS configuration for ingress.
  tls:
    - hosts:
        - llm.example.com
      secretName: anything-llm-tls

# -- Persistence configuration.
persistence:
  # -- Enable persistence.
  enabled: true
  # -- Access mode for the persistent volume.
  accessMode: ReadWriteOnce
  # -- Size of the persistent volume.
  size: 10Gi
  # -- List of volumes to create.
  volumes:
    - name: server-storage
      mountPath: /app/server/storage

# -- Configuration for the application.
config:
  STORAGE_DIR: "/app/server/storage"
  # -- Configuration for the embedding model.
  EMBEDDING_MODEL_PREF: "nomic-embed-text:1.5"
  EMBEDDING_MODEL_MAX_CHUNK_LENGTH: "8192"
  # -- Configuration for the vector db like lanceDB (in storage) or chroma DB (external), etc.
  VECTOR_DB: "lancedb"
    # ChromaDB: Enable all below if you are using chroma.
    # VECTOR_DB: chroma
    # CHROMA_ENDPOINT: http://anything-llm-chromadb:8000
  WHISPER_PROVIDER: "local"
  TTS_PROVIDER: "native"
  # -- Configuration for LLM provider like ollama, azure openai, etc.
# Ollama LLM and Embedding
  # LLM_PROVIDER='ollama'
  # OLLAMA_BASE_PATH: 'http://svc-name.namespace.svc.cluster.local:11434'
  # OLLAMA_MODEL_PREF: 'llama2'
  # OLLAMA_MODEL_TOKEN_LIMIT=8192
  # EMBEDDING_BASE_PATH: 'http://svc-name.namespace.svc.cluster.local:11434'
# Azure OpenAI and Embedding
  # AZURE_OPENAI_ENDPOINT: https://replace-me.openai.azure.com/
  # OPEN_MODEL_PREF: "gpt-4o"
  # EMBEDDING_MODEL_PREF: "text-embedding-ada-002"
  # EMBEDDING_ENGINE: 'azure'
# OpenAI and Embedding
  # LLM_PROVIDER: 'openai'
  # OPEN_MODEL_PREF: 'gpt-4o-mini'
  # EMBEDDING_MODEL_PREF: "text-embedding-ada-002"
  # EMBEDDING_ENGINE: 'openai'


# -- Secret configuration.
secret:
  # -- Enable secrets.
  enabled: true
  # -- Name of the secret, if not set, a secret is generated.
  name: ""
  # -- Secret data.
  data:
    AUTH_TOKEN: "replace-me"
    JWT_SECRET: "replace-me"
  # Azure OpenAI
    # AZURE_OPENAI_KEY: "50f...."
  # OpenAI
    # OPEN_AI_KEY: "sk-...



###### CHROMADB ######
chromadb:
  enabled: false
  chromadb:
    auth:
      enabled: false
  service:
    type: ClusterIP

###### OLLAMA ######
ollama:
  enabled: false
  fullnameOverride: "ollama"
  ollama:
    gpu:
      enabled: true
    type: "nvidia"
    number: 1
    models:
      - gemma2
  tolerations:
    - key: "ai"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
  persistentVolume:
    enabled: true
    accessModes:
      - ReadWriteOnce
    size: 50Gi
    storageClass: ""
  autoscaling:
    enabled: false
    maxReplicas: 1
  image:
    repository: ollama/ollama
    tag: 0.3.12

###### NVIDIA DEVICE PLUGIN ######
nvidia-device-plugin:
  enabled: false
  fullnameOverride: "nvidia-device-plugin"
  resources:
    limits:
      nvidia.com/gpu: 1
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  # nodeSelector:
  #   nvidia.com/gpu: "true"